---
title: "STATS 216: HW3"
author: "Xiangpeng Li"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

1.   
    (a) Best subset will have the smallest training RSS. Best subset will search all the possibilities combinations and find the best ones which have the lowest training RSS.

    (b) More information needs to provide. We need to do cross validation or fit on the test data to get the test RSS.
    
    (c)
        i. true. As we move forward, we will add a new predictor with smallest RSS to the current set.
        
        ii. true. As we move forward, we will remove a new predictor with smallest RSS from the current set.
        
        iii. false. They have different directions to choose predictors so they are not related.
        
        iv. false. The same reason as above.
        
        v. false. Subset selection picks are indenpendent at each step.
        
2.
    (a) $\hat{g}$ will be a linear least square line across training data.
    
    (b) $\hat{g}$ will be a piecewise cubic polynomial with knots and continuous first derivative.
    
    (c) $\hat{g}$ will be a piecewise cubic polynomial with knots and continuous first and second derivatives.

    (d) $\hat{g}$ will be a piecewise cubic polynomial with knots and continuous first, second and third derivatives.
    
    (e) $\hat{g}$ will interpolate the training observation.
        
3.
    (a)
```{r}
setwd("D:/One Drive/OneDrive/Document/Study/Stanford/Introduction to Statistical Learning/homework/hw3")
load("body.RData")
par(mfrow = c(1, 2))
attach(Y)
boxplot(Height ~ Gender, main = "Height")
boxplot(Weight ~ Gender, main = "Weight")
```

On average, `1` has higher height and heavier weight, so we can assume `1` denotes male and `0` denotes female.
    
  (b)
```{r}
library(pls)
 
## set the seed to make your partition reproductible
set.seed(1)
traingIndex = sample(seq_len(nrow(X)), size = 307)

training = X[traingIndex, ]
trainRes = Y[traingIndex, ]

test = X[-traingIndex, ]
testRes = Y[-traingIndex, ]

pcr.fit = pcr(trainRes$Weight ~ ., data = training, scale = TRUE, validation = "CV")

plsr.fit = plsr(trainRes$Weight ~ ., data = training, scale = TRUE, validation = "CV")
```

We choose to scale our variables because we can ensure all variables are on the same unit Otherwise the scale on which the variables are measured will have an effect on the final PCR model.

  (c)
```{r}
summary(pcr.fit)
validationplot(pcr.fit, val.type = "MSEP", main = "PCR MSEP")

summary(plsr.fit)
validationplot(plsr.fit, val.type = "MSEP", main = "PLSR MSEP")

```

The lowest cross-validation occurs when $M = 17$ for PCR model and $M = 8$ for PLS model. Also we can see for the same number of components these two model are using, PLS tends to have slightly smaller cross-validation error and higher percentage of variance explained. That's because PLS searches for directions that explain variance in both predictors and response whereas PCR only considers predictors. However, it is just slight different in the two model (95.2% for PCR and 96.07% for PLS using 4 components), PLS made an small improvement on PCR predictions.


  (d)
  
As we can see from above figures descriping MESP for two models, MSEPs drop significantly around 2-4 components, and don't have a noticable changes as we inscrease the number of components, we can see the same pattern for cross-validation error and % variance explained. So we choose $M = 4$ to make predictions.
```{r}
pcr.pred = predict(pcr.fit, newdata = test, ncomp = 4);

mean((pcr.pred - testRes)^2)


plsr.pred = predict(plsr.fit, newdata = test, ncomp = 4);

mean((plsr.pred - testRes)^2)
```

The test MSE for PLS model is bit smaller than PCR test MSE which provide another evidence that PLS is slightly better than PCR in this scenario.

  (e)
  
PCR and PLS are not feature selection models, it's because each of the components will be using a linear combination of all features. A typical feature selection model is LASSO, so we can use LASSO to fit this data to improve interpretion.
```{r}
library(glmnet)
dataFrame = data.frame("Weight" = Y$Weight, X)
x = model.matrix(Weight ~ ., dataFrame[traingIndex,])[, -1]
xTest = model.matrix(Weight ~ ., dataFrame[-traingIndex,])[, -1]

# fit the lasso model
lasso.fit = glmnet(x, trainRes$Weight, alpha = 1)
plot(lasso.fit)

# use cross-validation to get the tuning parameter
cv.out = cv.glmnet(x, trainRes$Weight, alpha = 1)
bestlam = cv.out$lambda.min

lasso.coef = predict(lasso.fit, s = bestlam, type = "coefficients")
# coefficient estimates using cross-validation
lasso.coef
```

(f)
```{r}
# predictions using PCR
pcr.pred = predict(pcr.fit, newdata = test, ncomp = 4);
pcr.mse = mean((pcr.pred - testRes)^2)


# predictions using PLS
plsr.pred = predict(plsr.fit, newdata = test, ncomp = 4);
plsr.mse = mean((plsr.pred - testRes)^2)

# precitions using LASSO
lasso.pred = predict(lasso.fit, s = bestlam, newx = xTest)
lasso.mse = mean((lasso.pred - testRes)^2)

dataFrame = data.frame(matrix(ncol = 3, nrow = 0))
colnames(dataFrame) = c("PCR", "PLS", "LASSO")

dataFrame[1, ] = c(pcr.mse, plsr.mse, lasso.mse)
print(dataFrame)
```

Although LASSO only using 18 out of 21 features, it doesn't have a better predictions. Results show these three model have the similar performance. PLS has the lowest test MSE followed by LASSO, PCR has the highest test MSE.


4.
(a)
```{r}
h = function(x, z) {
  if (x > z) (x - z)^3
  else 0
}
```

(b)
```{r}
hs = function(xs, z) {
  sapply(xs, function (x) h(x, z))
}
```

(c)
```{r}
splinebasis = function(xs, zs) {
  row = length(xs);
  col = length(zs) + 3;
  res = matrix(nrow = row, ncol = col);
  res[, 1] = xs;
  res[, 2] = xs^2;
  res[, 3] = xs^3;
  for (i in 1:length(zs)) {
    res[, i+3] = hs(xs, zs[i]);
  }
  res;
}
```

(d)
```{r}
set.seed(1337)
x = runif(100)
y = sin(10 * x)
```

(e)
```{r}
knots = c(1/4, 2/4, 3/4)
dataFrame = data.frame(y, splinebasis(x, knots))

# fit
lm.fit = lm(y ~ ., data = dataFrame)


intvl = seq(0, 1, len = 100)
nx = splinebasis(intvl, knots)
test = data.frame(nx)

lm.pred = predict(lm.fit, newdata = test)


plot(x, y)
curve(sin(10*x), 0, 1, add = TRUE, col = "red")
lines(intvl, lm.pred, col = "green")
```

green line represents the fitted spline, each black circle represents the original points and red line represents the curve those points were generated from

(f)
```{r}
par(mfrow = c(1, 2))
for (k in 1:9) {
  knots = 1:k
  knots = knots / (k + 1)
  dataFrame = data.frame(y, splinebasis(x, knots))
  lm.fit = lm(y ~ ., data = dataFrame)
  nx = splinebasis(intvl, knots)
  test = data.frame(nx)
  lm.pred = predict(lm.fit, newdata = test)
  plot(x, y, main = paste(k, "knots"))
  curve(sin(10*x), 0, 1, add = TRUE, col = "red")
  lines(intvl, lm.pred, col = "green")
}
```

(g)
Theoretically as we increase the number of knots, the fitted curve will continually approximate to the true curve and eventually interpolate the true curve. But in this case when knots are greater than 4, we can barely see the difference between these two curves.

