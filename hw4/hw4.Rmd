---
title: "STATS 216: HW4"
author: "Xiangpeng Li"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

2.   

(a)
```{r}
set.seed(1)
x = matrix(rnorm(25 * 3 * 45, mean = 0), ncol = 45)

# shift mean
x[1:25, 1:45] = x[1:25, 1:45] + 3
x[26:50, 1:45] = x[26:50, 1:45] - 3
x[51:75, 1:45] = x[51:75, 1:45] + 7
```

(b)
```{r}
# perform PCA
pca.fit = prcomp(x, scale = TRUE)
biplot(pca.fit, scale = 0)
```

(c)
```{r}
# perform K-means
kmeans3 = kmeans(x, 3, nstart = 20)

plot(x, col = (kmeans3$cluster + 1), pch = 20, cex = 2)
table(kmeans3$cluster, c(rep(2, 25), rep(1, 25), rep(3, 25)))
```
Since K-means will arbitrarily number the cluster, the label may not be correct, but we can clearly see the predictors are correctly clustered,

(d)
```{r}
kmeans2 = kmeans(x, 2, nstart = 20)

plot(x, col = (kmeans2$cluster + 1), pch = 20, cex = 2)
table(kmeans2$cluster, c(rep(2, 25), rep(1, 25), rep(3, 25)))
```
As we can see in the plot and table, all the observations in one group are fully labeled as another group.

(e)
```{r}
kmeans4 = kmeans(x, 4, nstart = 20)

plot(x, col = (kmeans4$cluster + 1), pch = 20, cex = 2)
table(kmeans4$cluster, c(rep(2, 25), rep(1, 25), rep(3, 25)))
```
In this fit, observations from top right are divided into two clusters.

(f)
```{r}
kmeans.pca.fit = kmeans(pca.fit$x[1:75, 1:2], 3, nstart = 20)

plot(x, col = (kmeans.pca.fit$cluster + 1), pch = 20, cex = 2)
table(kmeans.pca.fit$cluster, c(rep(2, 25), rep(1, 25), rep(3, 25)))
```
All the observations are correctly labeled. It means that first and second components explain the majority of the observations.

(g)
```{r}
kmeansScale = kmeans(scale(x), 4, nstart = 20)

plot(x, col = (kmeansScale$cluster + 1), pch = 20, cex = 2)
table(kmeansScale$cluster, c(rep(2, 25), rep(1, 25), rep(3, 25)))
```
Some observations are not labeled correctly. Because each pairwise distance will be affected by the scale function. If the measure units are already the same, we should not scale it again.


3.

(a)
```{r}
# setup data
setwd("D:/One Drive/OneDrive/Document/Study/Stanford/Introduction to Statistical Learning/homework/hw4")
load("body.RData")
set.seed(1)
traingIndex = sample(seq_len(nrow(X)), size = 307)

training = X[traingIndex, ]
trainRes = Y[traingIndex, ]

test = X[-traingIndex, ]
testRes = Y[-traingIndex, ]

# fit the model
library(randomForest)

bag.mse = numeric(50)
rf.mse = numeric(50)
xvalue = c(1:50) * 10

for (i in 1:50) {
  bag.fit = randomForest(trainRes$Weight ~ ., data = training, ntree = i * 10, mtry = ncol(training), importance = TRUE)
  rf.fit = randomForest(trainRes$Weight ~ ., data = training, ntree = i * 10, mtry = ncol(training) / 3, importance = TRUE)
  
  bag.pred = predict(bag.fit, newdata = test)
  rf.pred = predict(rf.fit, newdata = test)
  
  bag.mse[i] = mean((bag.pred - testRes$Weight)^2)
  rf.mse[i] = mean((rf.pred - testRes$Weight)^2)
}
plot(xvalue, bag.mse, xlab = "Number of Trees", ylab = "MSE", col = "blue", pch = 19, type = "l", ylim = c(9.5, 17))
points(xvalue, rf.mse, pch = 19, type = "l")
```

(b)
```{r}
bag.fit = randomForest(trainRes$Weight ~ ., data = training, mtry = ncol(training), importance = TRUE)
rf.fit = randomForest(trainRes$Weight ~ ., data = training, mtry = ncol(training) / 3, importance = TRUE)
importance(bag.fit)
importance(rf.fit)
```
For Bagging, Waist.Girth has the largest %IncMSE and IncNodePurity, so it's the most important variable. Fot Random forest, Hip.Girth and Waist.Girth are the most importance variables, they have the largest %IncMSE and IncNodePurity.

(c)

```{r}
rf.fit = randomForest(trainRes$Weight ~ ., data = training, mtry = ncol(training) / 3, importance = TRUE, ntree = 500)
rf.pred = predict(rf.fit, newdata = test)
rf.mse = mean((rf.pred - testRes$Weight)^2)
rf.mse1 = mean((rf.pred - testRes)^2)
rf.mse
rf.mse1
```
In homework 3(f) I'm using the whole test variables instead of Weight only to calculate MSE, so the numbers are off. But still we can use the same apporoach to calculate the MSE so we can have a basic idea of their performance. 

PCR: 4281.326, PLS: 4279.822, LASSO: 4280.238, RF: 4268.863. Random forest have the lowest MSE so it has the best performance.

(d)
As we can see from the plot in (a), both Bagging and random forest don't have much difference when the numbers of trees are above 100, so the default setting is enough for us in this scenario.

4

(a)
```{r}
x1 = c(3, 2, 4, 1, 2, 4, 4)
x2 = c(4, 2, 4, 4, 1, 3, 1)
y = c("red", "red", "red", "red", "blue", "blue", "blue")

plot(x1, x2, col = y, type = "p")

```

(b)
As we can see from above figure, points $(2, 1), (2, 2), (4, 4), (4, 3)$ are the supporting vector, the boundary has to be within that region. In order to get the maximum margin, we can conclude the line has to pass through $(2, 1.5), (4, 3.5)$. So the equation will be $$-0.5 + x1 - x2 = 0$$
```{r}
plot(x1, x2, col = y, type = "p")
abline(a = -0.5, b = 1)
```


(c)

$\beta_0 = -0.5, \beta_1 = 1, \beta_2 = -1$
Classify to Red if $-0.5 + x1 - x2 = 0 > 0$ and classify to blue otherwise

(d)
```{r}
plot(x1, x2, col = y, type = "p")
abline(a = -1, b = 1, col = "green", lty = 2)
abline(a = 0, b = 1, col = "green", lty = 2)
abline(a= -0.5, b = 1)
```
Then we calculate the distance between the solid black line and the dotted green line $0.5 - x1 - x2 = 0$ and $-1 + x1 - x2 = 0$. Through the distance formala we get the distance is $\sqrt{2} / 4$

(e)

points $(2, 1), (2, 2), (4, 4), (4, 3)$ are the supporting vectors

(f)

It will not affect the maximal margin hyperplane. As we can see from the above figure, point $(4, 1)$ is far from the maximal margin classifier line. As long as it's not moving into the margin, it has no affect on the hyperplane.

(g)
```{r}
plot(x1, x2, col = y, type = "p")
abline(a= -0.5, b = 1)
abline(a= -0.6, b = 1, col = "yellow")
```

I drawed a yellow line with different intercept(0.6), the equation is: $-0.6 + x1 - x2 = 0 > 0$

(h)

```{r}
plot(x1, x2, col = y, type = "p")
points(3, 1.5, col = "red")
```

I drawed a point at (3, 1.5) in red. In this case they cannot be separated by a hyperplane.
